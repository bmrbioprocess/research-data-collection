# --- 1. IMPORTS AND INITIAL CONFIGURATION ---
import os
import re
import time
import logging
from collections import Counter
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple, Any
import pandas as pd
import requests
import spacy
from dotenv import load_dotenv
import diskcache as dc

load_dotenv()
# Configuração única de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'API_Beatriz_{datetime.now().strftime("%Y%m%d_%H%M")}.log'),
        logging.StreamHandler()
    ]
)

# --- 2. GLOBAL CONSTANTS AND CONFIGURATIONS ---
class Config:
    """Global configurations and constants for English scientific article processing."""
    SCOPUS_API_KEY = os.getenv("SCOPUS_API_KEY")
    SCOPUS_API_URL = "https://api.elsevier.com/content/abstract/doi/"
    MDPI_API_URL = "https://www.mdpi.com/search/json_search"
    CROSSREF_API_URL = "https://api.crossref.org/works/"

    REQUEST_TIMEOUT = 30
    REQUEST_DELAY = 1
    MAX_RETRIES = 3
    MAX_WORKERS = 3
    CACHE_EXPIRY = 86400

    POSITIVE_INDICATORS = [
        'was used', 'was studied', 'was cultured', 'was grown', 'the strain',
        'this organism', 'inoculated', 'employed', 'selected for', 'utilized',
        'investigated', 'chosen', 'subjected to', 'cultivated in', 'grown in', 'fermented'
    ]
    NEGATIVE_INDICATORS = [
        'compared to', 'similar to', 'in contrast', 'unlike', 'previously reported for',
        'as opposed to', 'different from', 'in comparison with', 'relative to', 'versus'
    ]
    PRODUCT_KEYWORDS_NORMALIZED = {
        'Acetate': ['acetate', 'acetic acid'], 'Ethanol': ['ethanol'], 'Butanol': ['butanol', 'n-butanol'],
        'Hexanol': ['hexanol', 'n-hexanol', '1-hexanol'], 'Butyrate': ['butyrate', 'butyric acid'],
        'Caproate': ['caproate', 'caproic acid'], '2,3-Butanediol': ['2,3-butanediol', '2,3 butanediol'],
        'Isopropanol': ['isopropanol'], 'Formate': ['formate', 'formic acid'],
        'Lactate': ['lactate', 'lactic acid'], 'Propionate': ['propionate'], 'Octanol': ['octanol'],
        'Heptanol': ['heptanol'], 'Pentanol': ['pentanol'], 'Valerate': ['valerate']
    }
    NOMINAL_GAS_KEYWORDS = {
        'CO': ['carbon monoxide'], 'H2': ['hydrogen'],
        'CO2': ['carbon dioxide'], 'CH4': ['methane']
    }
    GAS_COMPOSITION_PATTERNS = [
        r'(\d{1,3}\.?\d?)%\s*(H2|CO2|CO|CH4|N2|O2|H₂|CO₂|CH₄|N₂|O₂)',
        r'(?:ratio|proportion)\s+of\s+(\d+)\s*:\s*(\d+).*?(H2.*?CO2|CO2.*?H2|CO.*?H2)',
    ]
    CULTURE_MEDIUM_PATTERNS = [
        r'[^.]*?(?:medium contained|composed of|consisted of|medium composition|basal medium|growth medium)[^.]*\.',
        r'[^.]*?(?:supplemented with|added with|containing)[^.]*?(\d+\.?\d*\s*(?:g/L|mg/L|mM|%))[^.]*\.',
    ]

# --- 3. UTILITY FUNCTIONS ---
def calculate_ordinatio(citations: int, year: int, research_year: int, alpha: float = 10) -> float:
    """Calculates the balanced Ordinatio index."""
    age = research_year - year
    age_factor = max(0, 10 - age)
    ordinatio = (citations / 10) + (alpha * age_factor)
    return round(ordinatio, 2)

# --- 4. API SEARCH STRATEGIES ---
class APISearchStrategy:
    def search(self, doi: str, session: requests.Session) -> Tuple[Optional[Dict], Optional[str]]:
        raise NotImplementedError

class ScopusSearchStrategy(APISearchStrategy):
    def search(self, doi: str, session: requests.Session) -> Tuple[Optional[Dict], Optional[str]]:
        headers = {"X-ELS-APIKey": Config.SCOPUS_API_KEY, "Accept": "application/json"}
        url = f"{Config.SCOPUS_API_URL}{doi}"
        response = self._make_request(session, url, headers=headers)
        if response and response.status_code == 200:
            return response.json(), "Scopus"
        elif response:
            logging.error(f"Scopus returned an error for DOI {doi}. Status: {response.status_code} | Response: {response.text}")
        return None, None
        
    def _make_request(self, session: requests.Session, url: str, **kwargs) -> Optional[requests.Response]:
        """Make request with intelligent retry mechanism."""
        for attempt in range(Config.MAX_RETRIES):
            try:
                response = session.get(url, timeout=Config.REQUEST_TIMEOUT, **kwargs)
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 429:  # Rate limiting
                    wait_time = 2 ** attempt + 1
                    logging.warning(f"Rate limited. Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                elif 500 <= response.status_code < 600:  # Server errors
                    logging.warning(f"Server error {response.status_code}. Retrying...")
                    time.sleep(2 ** attempt)
                else:
                    return response  # Other client errors (401, 403, 404) are not retried
                    
            except requests.exceptions.RequestException as e:
                logging.error(f"Connection error for {url} (attempt {attempt + 1}): {e}")
                if attempt == Config.MAX_RETRIES - 1:
                    break
                    
            if attempt < Config.MAX_RETRIES - 1:
                time.sleep(2 ** attempt)
                
        return None

class MDPISearchStrategy(APISearchStrategy):
    def search(self, doi: str, session: requests.Session) -> Tuple[Optional[Dict], Optional[str]]:
        params = {'q': doi}
        response = self._make_request(session, Config.MDPI_API_URL, params=params)
        if response and response.status_code == 200:
            data = response.json()
            if data.get('hits') and data['hits'][0].get('doi') == doi: return data['hits'][0], "MDPI"
        return None, None
    def _make_request(self, session: requests.Session, url: str, **kwargs) -> Optional[requests.Response]:
        try: return session.get(url, timeout=Config.REQUEST_TIMEOUT, **kwargs)
        except requests.exceptions.RequestException as e:
            logging.error(f"MDPI error for {url}: {e}"); return None

class CrossrefSearchStrategy(APISearchStrategy):
    def search(self, doi: str, session: requests.Session) -> Tuple[Optional[Dict], Optional[str]]:
        url = f"{Config.CROSSREF_API_URL}{doi}"
        response = self._make_request(session, url)
        if response and response.status_code == 200: return response.json()['message'], "Crossref"
        return None, None
    def _make_request(self, session: requests.Session, url: str) -> Optional[requests.Response]:
        try: return session.get(url, timeout=Config.REQUEST_TIMEOUT)
        except requests.exceptions.RequestException as e:
            logging.error(f"Crossref error for {url}: {e}"); return None

# --- 5. CACHE SYSTEM ---
class APICache:
    def __init__(self, cache_dir: str = "./api_cache"): self.cache = dc.Cache(cache_dir)
    def get_metadata(self, doi: str) -> Optional[Dict]: return self.cache.get(doi)
    def set_metadata(self, doi: str, metadata: Dict, source: str):
        self.cache.set(doi, {'metadata': metadata, 'source': source, 'timestamp': datetime.now().isoformat()}, expire=Config.CACHE_EXPIRY)
    def close(self): self.cache.close()

# --- 6. METADATA ADAPTERS ---
class MetadataAdapter:
    @staticmethod
    def adapt_scopus(data: Dict) -> Dict:
        abstract_info = data.get('abstracts-retrieval-response', {})
        coredata = abstract_info.get('coredata', {})
        authors = abstract_info.get('authors', {}).get('author', [])
        author_keywords = []
        authkeywords_data = abstract_info.get('authkeywords', {})
        if authkeywords_data:
            keyword_list = authkeywords_data.get('author-keyword', [])
            if isinstance(keyword_list, dict): keyword_list = [keyword_list]
            author_keywords = [kw.get('$', '') for kw in keyword_list if kw.get('$')]
        authkeywords_str = "; ".join(author_keywords)
        return {'title': coredata.get('dc:title'), 'authors': "; ".join([a.get('ce:indexed-name', '') for a in authors]),
                'year': coredata.get('prism:coverDate', '').split('-')[0] if coredata.get('prism:coverDate') else None,
                'journal': coredata.get('prism:publicationName'), 'abstract': coredata.get('dc:description'),
                'doi': coredata.get('prism:doi'), 'source': 'Scopus', 'citations': int(coredata.get('citedby-count', 0)),
                'author_keywords': authkeywords_str}
    @staticmethod
    def adapt_mdpi(data: Dict) -> Dict:
        return {'title': data.get('title'), 'authors': data.get('authors', '').replace(', ', '; '), 'year': str(data.get('year')) if data.get('year') else None, 'journal': data.get('journal'), 'abstract': data.get('abstract'), 'doi': data.get('doi'), 'source': 'MDPI'}
    @staticmethod
    def adapt_crossref(data: Dict) -> Dict:
        authors = [f"{a.get('given', '')} {a.get('family', '')}".strip() for a in data.get('author', [])]
        abstract = data.get('abstract', '')
        if isinstance(abstract, dict): abstract = abstract.get('value', '')
        return {'title': data.get('title', [None])[0], 'authors': "; ".join(authors), 'year': str(data.get('created', {}).get('date-parts', [[None]])[0][0]), 'journal': data.get('container-title', [None])[0], 'abstract': abstract, 'doi': data.get('DOI'), 'source': 'Crossref'}
    
    @staticmethod
    def validate_metadata(metadata: Dict) -> bool:
        """Robust validation of metadata."""
        required = ['title', 'doi']
        for field in required:
            if not metadata.get(field) or not str(metadata[field]).strip():
                return False
        
        doi = metadata.get('doi', '').strip()
        
        if not doi.startswith('10.'):
            logging.warning(f"Invalid DOI format (does not start with '10.'): {doi}")
            return False
        
        if len(doi) < 10:
            logging.warning(f"DOI appears too short: {doi}")
            return False
            
        if '/' not in doi or len(doi.split('/')[1]) < 2:
            logging.warning(f"DOI structure invalid: {doi}")
            return False
            
        return True

# --- 7. ADVANCED NLP PROCESSING ---
class ScientificNPLExtractor:
    def __init__(self, nlp_model):
        self.nlp = nlp_model
        custom_stopwords = {"study", "results", "method", "conclusion", "paper", "article", "introduction", "background", "abstract", "objective", "purpose"}
        for word in custom_stopwords: self.nlp.vocab[word].is_stop = True

    def extract_keywords_from_text(self, title: str, abstract: str, num_keywords: int = 10) -> List[str]:
        if not abstract and not title: return []
        text_source = abstract if abstract else title
        full_text = ((title or "").lower() + ' ') * 2 + (text_source or "").lower()
        doc = self.nlp(full_text)
        filtered_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space and not token.like_num]
        word_freq = Counter(filtered_tokens)
        return [word for word, freq in word_freq.most_common(num_keywords)]

    def extract_microorganisms_advanced(self, abstract_text: str, title_text: str) -> Tuple[str, Counter]:
        full_text = (title_text or "") + ". " + (abstract_text or "")
        pattern = r'\b(Clostridium|C\.)\s+([a-z]+[a-z\-]*[a-z]+)\b'
        raw_occurrences = re.findall(pattern, full_text, re.IGNORECASE)
        if not raw_occurrences: return "", Counter()
        formatted_candidates = [f"{'C.' if g.lower() == 'c.' else 'Clostridium'} {s.lower()}" for g, s in raw_occurrences]
        frequencies = Counter(formatted_candidates)
        unique_candidates = list(frequencies.keys())
        if not unique_candidates: return "", frequencies
        scores = self._calculate_microorganism_scores(unique_candidates, frequencies, abstract_text, title_text)
        main_microorganism = max(scores, key=scores.get) if scores else ""
        return main_microorganism, frequencies
    
    def _calculate_microorganism_scores(self, candidates: List[str], freqs: Counter, abstract: str, title: str) -> Dict[str, int]:
        scores = {c: 0 for c in candidates}
        for candidate in candidates:
            species = candidate.split(' ')[1] if ' ' in candidate else candidate
            scores[candidate] += freqs[candidate]
            if title and re.search(r'\b' + re.escape(species) + r'\b', title, re.IGNORECASE): scores[candidate] += 5
            if abstract:
                doc = self.nlp(abstract)
                for sent in doc.sents:
                    text = sent.text.lower()
                    if re.search(r'\b' + re.escape(species) + r'\b', text, re.IGNORECASE):
                        if any(ind in text for ind in Config.POSITIVE_INDICATORS): scores[candidate] += 3
                        if any(ind in text for ind in Config.NEGATIVE_INDICATORS): scores[candidate] -= 2
                        if self._is_subject_in_sentence(sent, species): scores[candidate] += 2
                        if self._is_near_experimental_terms(sent, species): scores[candidate] += 1
        return scores

    def _is_subject_in_sentence(self, sentence, species: str) -> bool:
        for token in sentence:
            if token.text.lower() == species.lower() and token.dep_ in ["nsubj", "nsubjpass"]: return True
        return False
    def _is_near_experimental_terms(self, sentence, species: str) -> bool:
        terms = ['fermentation', 'culture', 'growth', 'experiment', 'study', 'assay']
        s_tokens = [t for t in sentence if t.text.lower() == species.lower()]
        for st in s_tokens:
            for t in sentence:
                if t.text.lower() in terms and abs(t.i - st.i) <= 5: return True
        return False

    def extract_fermentation_products(self, text: str) -> List[str]:
        if not text: return []
        found = set()
        for name, synonyms in Config.PRODUCT_KEYWORDS_NORMALIZED.items():
            for synonym in synonyms:
                if re.search(r'\b' + re.escape(synonym) + r'\b', text, re.IGNORECASE):
                    found.add(name); break
        return sorted(list(found))

    def extract_gas_composition_advanced(self, text: str) -> List[str]:
        if not text: return []
        results = set()
        try:
            for pattern in Config.GAS_COMPOSITION_PATTERNS:
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    if match.group().strip(): results.add(match.group().strip())
            for formula, names in Config.NOMINAL_GAS_KEYWORDS.items():
                for name in names:
                    if re.search(r'\b' + re.escape(name) + r'\b', text, re.IGNORECASE): results.add(formula)
            if any(ind in text.lower() for ind in ['syngas', 'synthesis gas']):
                ratio = re.search(r'[Hh]2\s*[/:]\s*[Cc][Oo].*?(\d+\.?\d*)\s*:?\s*(\d+\.?\d*)', text)
                if ratio and ratio.group(1) is not None and ratio.group(2) is not None:
                    results.add(f"Syngas H2/CO {ratio.group(1)}:{ratio.group(2)}")
                else:
                    results.add("Syngas")
        except Exception as e:
            logging.error(f"Error during gas composition extraction: {e}")
        return sorted(list(results))

    def extract_culture_medium_advanced(self, text: str) -> List[str]:
        if not text: return []
        occurrences = []
        for pattern in Config.CULTURE_MEDIUM_PATTERNS: occurrences.extend(re.findall(pattern, text, re.IGNORECASE))
        return list(set([s.strip() for s in occurrences]))

    def process_abstract_comprehensive(self, abstract_text: str, title_text: str) -> Dict:
        main_microorganism, species_count = self.extract_microorganisms_advanced(abstract_text, title_text)
        fermentation_products = self.extract_fermentation_products(abstract_text)
        gas_composition = self.extract_gas_composition_advanced(abstract_text)
        culture_medium = self.extract_culture_medium_advanced(abstract_text)
        nlp_keywords = self.extract_keywords_from_text(title_text, abstract_text)
        return {'main_microorganism': main_microorganism or "Not extracted",'all_species_count': species_count,
                'fermentation_products': "; ".join(fermentation_products) or "Not extracted",
                'gas_composition': "; ".join(gas_composition) or "Not extracted",
                'culture_medium': "; ".join(culture_medium) or "Not extracted",
                'nlp_keywords': "; ".join(nlp_keywords) or "Not extracted"}

# --- 8. PROCESSING MANAGER ---
class ScientificProcessingManager:
    def __init__(self, nlp_model):
        self.cache = APICache()
        self.nlp_extractor = ScientificNPLExtractor(nlp_model)
        self.search_strategies = [ScopusSearchStrategy(), MDPISearchStrategy(), CrossrefSearchStrategy()]
        self.metadata_adapter = MetadataAdapter()
        self.report_data = {
            'start_time': time.time(), 
            'total_processed': 0, 
            'metadata_success': 0, 
            'total_failures': 0,
            'main_microorganism_extractions': 0, 
            'gas_extractions': 0, 
            'medium_extractions': 0,
            'product_extractions': 0, 
            'processing_times': [], 
            'dois_not_found': [], 
            'processing_errors': [],
            'source_distribution': Counter(), 
            'all_species_counts': []
        }

    def fetch_metadata(self, doi: str, session: requests.Session) -> Tuple[Optional[Dict], Optional[str]]:
        cached_data = self.cache.get_metadata(doi)
        if cached_data:
            logging.info(f"  - Cache hit for {doi}")
            return cached_data['metadata'], cached_data['source']
        
        for strategy in self.search_strategies:
            raw_data, source = strategy.search(doi, session)
            if raw_data:
                adapters = {
                    'Scopus': self.metadata_adapter.adapt_scopus, 
                    'MDPI': self.metadata_adapter.adapt_mdpi, 
                    'Crossref': self.metadata_adapter.adapt_crossref
                }
                if source in adapters:
                    metadata = adapters[source](raw_data)
                    if self.metadata_adapter.validate_metadata(metadata):
                        self.cache.set_metadata(doi, metadata, source)
                        return metadata, source
        return None, None

    def process_single_doi(self, doi: str) -> Dict:
        start_time = time.time()
        try:
            with requests.Session() as session:
                metadata, source = self.fetch_metadata(doi, session)
                if metadata and source:
                    logging.info(f"  - Metadata found via {source}")
                    nlp_data = self.nlp_extractor.process_abstract_comprehensive(
                        metadata.get('abstract'), metadata.get('title')
                    )
                    result = {**metadata, **nlp_data}
                    self._update_processing_metrics(nlp_data, source)
                else:
                    logging.warning(f"  - DOI not found: {doi}")
                    result = {'doi': doi, 'title': 'NOT FOUND'}
                    self.report_data['total_failures'] += 1
                    self.report_data['dois_not_found'].append(doi)
            
            processing_time = time.time() - start_time
            self.report_data['processing_times'].append(processing_time)
            time.sleep(Config.REQUEST_DELAY)
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.report_data['processing_times'].append(processing_time)
            raise e

    def _update_processing_metrics(self, nlp_data: Dict, source: str):
        self.report_data['metadata_success'] += 1
        self.report_data['source_distribution'][source] += 1
        
        if nlp_data.get('main_microorganism') != "Not extracted": 
            self.report_data['main_microorganism_extractions'] += 1
        if nlp_data.get('gas_composition') != "Not extracted": 
            self.report_data['gas_extractions'] += 1
        if nlp_data.get('culture_medium') != "Not extracted": 
            self.report_data['medium_extractions'] += 1
        if nlp_data.get('fermentation_products') != "Not extracted": 
            self.report_data['product_extractions'] += 1
        if nlp_data.get('all_species_count'): 
            self.report_data['all_species_counts'].append(nlp_data['all_species_count'])

    def process_dois_parallel(self, dois_list: List[str]) -> List[Dict]:
        results = []
        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
            future_to_doi = {executor.submit(self.process_single_doi, doi): doi for doi in dois_list}
            for i, future in enumerate(as_completed(future_to_doi)):
                doi = future_to_doi[future]
                try:
                    result = future.result()
                    results.append(result)
                    logging.info(f"Progress: {i+1}/{len(dois_list)} - DOI: {doi} processed.")
                except Exception as e:
                    logging.error(f"An unexpected error occurred while processing {doi}: {e}")
                    self.report_data['total_failures'] += 1
                    self.report_data['processing_errors'].append({'doi': doi, 'error': str(e)})
                    results.append({'doi': doi, 'title': 'PROCESSING ERROR', 'error': str(e)})
        return results
    
    def validate_scientific_extractions(self, results: List[Dict]) -> List[str]:
        issues = []
        for result in results:
            if result.get('title') in ['NOT FOUND', 'PROCESSING ERROR']: 
                continue
            year = result.get('year')
            if year is not None:
                year_str = str(year).strip()
                if not year_str.isdigit():
                    issues.append(f"Invalid year format in {result['doi']}: {year}")
                else:
                    year_int = int(year_str)
                    if not (1900 <= year_int <= datetime.now().year):
                        issues.append(f"Year out of range in {result['doi']}: {year_int}")
        return issues
    
# --- 9. REPORT GENERATOR ---
class ScientificReportGenerator:
    @staticmethod
    def save_results_excel(results: List[Dict], script_path: str):
        if not results: logging.warning("No data to save."); return
        df = pd.DataFrame(results)
        df_clean = df.drop(columns=[col for col in ['author_keywords'] if col in df.columns])
        if 'all_species_count' in df_clean.columns:
            df_clean['species_count_str'] = df_clean['all_species_count'].apply(lambda x: "; ".join([f"{k}:{v}" for k,v in x.items()]) if isinstance(x, Counter) else "")
        filename = f"scientific_extraction_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        full_path = os.path.join(script_path, filename)
        try:
            df_clean.to_excel(full_path, index=False, engine='openpyxl')
            logging.info(f"Excel file saved: {full_path}")
        except Exception as e: logging.error(f"Error saving Excel: {e}")

    @staticmethod
    def gerar_planilha_ordinatio(lista_resultados: List[Dict], script_path: str):
        logging.info("Generating Ordinatio spreadsheet...")
        resultados_scopus = [res for res in lista_resultados if (res.get('source') == 'Scopus' and res.get('year') and res.get('title') not in ['NOT FOUND', 'PROCESSING ERROR'])]
        if not resultados_scopus:
            logging.warning("No valid Scopus data found for Ordinatio spreadsheet."); return
        try:
            df = pd.DataFrame(resultados_scopus)
            if 'citations' not in df.columns: df['citations'] = 0
            current_year = datetime.now().year
            df['year'] = pd.to_numeric(df['year'], errors='coerce')
            df['citations'] = pd.to_numeric(df['citations'], errors='coerce').fillna(0)
            df = df.dropna(subset=['year'])
            df['year'] = df['year'].astype(int)
            df = df[(df['year'] >= 1900) & (df['year'] <= current_year)]
            if df.empty:
                logging.warning("No valid data after cleaning for Ordinatio."); return
            
            df['InOrdinatio'] = df.apply(lambda row: calculate_ordinatio(row['citations'], row['year'], current_year), axis=1)
            df['Keywords'] = df.apply(lambda x: x['author_keywords'] if x.get('author_keywords') and str(x['author_keywords']).strip() else x.get('nlp_keywords', ''), axis=1)
            df_renamed = df.rename(columns={'title': 'Title', 'authors': 'Authors', 'doi': 'DOI', 'year': 'Publication Year', 'citations': 'Citations', 'journal': 'Journal'})
            columns_order = ['Title', 'Authors', 'DOI', 'Keywords', 'Journal', 'Publication Year', 'Citations', 'InOrdinatio']
            df_final = df_renamed.reindex(columns=[col for col in columns_order if col in df_renamed.columns])
            nome_arquivo = f"Ordinatio_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            caminho_completo = os.path.join(script_path, nome_arquivo)
            df_final.to_excel(caminho_completo, index=False, engine='openpyxl')
            logging.info(f"Ordinatio spreadsheet saved successfully: {caminho_completo}")
        except Exception as e:
            logging.error(f"Failed to generate Ordinatio spreadsheet: {e}")
            
    @staticmethod
    def generate_execution_report(report_data: Dict, script_path: str, validation_issues: List[str]):
        total_time = time.time() - report_data['start_time']
        success_count = report_data['metadata_success']
        total_processed = report_data['total_processed']
        
        times = report_data['processing_times']
        if times:
            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)
        else:
            avg_time = min_time = max_time = 0
        
        success_rate = (success_count / total_processed * 100) if total_processed > 0 else 0
        agg_species = Counter()
        for count in report_data['all_species_counts']: agg_species.update(count)
        filename = f"scientific_execution_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        full_path = os.path.join(script_path, filename)
        with open(full_path, "w", encoding="utf-8") as f:
            f.write("="*70 + "\nSCIENTIFIC EXTRACTION EXECUTION REPORT - API Beatriz\n" + "="*70 + "\n\n")
            f.write("### GENERAL METRICS ###\n")
            f.write(f"Total execution time: {total_time:.2f} seconds\n")
            f.write(f"Average time per article: {avg_time:.2f} seconds\n")
            f.write(f"Minimum processing time: {min_time:.2f} seconds\n")
            f.write(f"Maximum processing time: {max_time:.2f} seconds\n")
            f.write(f"DOIs processed: {total_processed}\n")
            f.write(f"Successfully retrieved: {success_count} ({success_rate:.1f}%)\n")
            f.write(f"Failures: {report_data['total_failures']}\n\n")
            f.write("### SOURCE DISTRIBUTION ###\n")
            for source, count in report_data['source_distribution'].items(): f.write(f"- {source}: {count}\n")
            f.write("\n### NLP EXTRACTION EFFECTIVENESS ###\n")
            f.write(f"- Main microorganisms: {report_data['main_microorganism_extractions']}\n")
            f.write(f"- Fermentation products: {report_data['product_extractions']}\n")
            f.write(f"- Gas compositions: {report_data['gas_extractions']}\n")
            f.write(f"- Culture media: {report_data['medium_extractions']}\n\n")
            f.write("### MOST MENTIONED MICROORGANISMS ###\n")
            for org, count in agg_species.most_common(10): f.write(f"- {org}: {count}\n")
            if validation_issues:
                f.write("\n### DATA VALIDATION ISSUES ###\n")
                for issue in validation_issues: f.write(f"- {issue}\n")
            f.write("\n### FAILED OR UNRESOLVED DOIs ###\n")
            if report_data['dois_not_found']:
                f.write("\nNot found in any source:\n")
                for doi in report_data['dois_not_found']: f.write(f"- {doi}\n")
            if report_data['processing_errors']:
                f.write("\nCaused a processing error:\n")
                for err in report_data['processing_errors']: f.write(f"- DOI: {err['doi']} | Error: {err['error']}\n")
        logging.info(f"Execution report saved: {full_path}")

# --- 10. MAIN EXECUTION FUNCTION ---
def main():
    """Main function with comprehensive error handling."""
    print("Starting API Beatriz - Scientific Extraction System...")
    if not Config.SCOPUS_API_KEY or Config.SCOPUS_API_KEY.strip() == "":
        logging.error("Scopus API key not found or empty in .env file"); return
    try:
        nlp_model = spacy.load("en_core_web_sm")
        logging.info("spaCy model loaded successfully.")
    except OSError as e:
        logging.error(f"spaCy model not found: {e}\nPlease run: python -m spacy download en_core_web_sm"); return
    except Exception as e:
        logging.error(f"Unexpected error loading spaCy model: {e}"); return
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    manager = None
    try:
        manager = ScientificProcessingManager(nlp_model)
        EXCEL_FILENAME = "entrada_dois.xlsx"
        SHEET_NAME = "Selecionados Scopus"
        file_path = os.path.join(script_dir, EXCEL_FILENAME)
        if not os.path.exists(file_path):
            logging.error(f"Input file not found: {file_path}"); return
        logging.info(f"Reading spreadsheet: {file_path}")
        try:
            df_dois = pd.read_excel(file_path, sheet_name=SHEET_NAME)
        except ValueError:
            logging.warning(f"Sheet '{SHEET_NAME}' not found. Trying to read the first sheet instead.")
            try: df_dois = pd.read_excel(file_path, sheet_name=0)
            except Exception as e_inner:
                logging.error(f"Could not read any sheet from the Excel file: {e_inner}"); return
        if 'DOI' not in df_dois.columns:
            logging.error("Column 'DOI' not found in spreadsheet."); return
        dois_list = df_dois['DOI'].dropna().tolist()
        if not dois_list:
            logging.warning("No DOIs to process."); return
        
        manager.report_data['total_processed'] = len(dois_list)
        logging.info(f"Starting processing of {len(dois_list)} articles...")
        results = manager.process_dois_parallel(dois_list)
        validation_issues = manager.validate_scientific_extractions(results)
        ScientificReportGenerator.save_results_excel(results, script_dir)
        ScientificReportGenerator.generate_execution_report(manager.report_data, script_dir, validation_issues)
        ScientificReportGenerator.gerar_planilha_ordinatio(results, script_dir)
        logging.info("Scientific processing completed successfully!")
    except FileNotFoundError as e: logging.error(f"File not found: {e}")
    except ValueError as e: logging.error(f"Data validation error: {e}")
    except Exception as e: logging.error(f"Unexpected error in main execution: {e}")
    finally:
        if manager: manager.cache.close()

if __name__ == "__main__":
    main()
